"""
ç”Ÿæˆå®éªŒæŠ¥å‘Šæ‰€éœ€çš„å¯è§†åŒ–å›¾è¡¨
"""

import matplotlib.pyplot as plt
import matplotlib
import numpy as np
import os
import seaborn as sns

# è®¾ç½®ä¸­æ–‡å­—ä½“
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False

# åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs('figures', exist_ok=True)

# è®¾ç½®é£æ ¼
plt.style.use('seaborn-v0_8-whitegrid')
colors = ['#2ecc71', '#3498db', '#e74c3c', '#9b59b6', '#f39c12', '#1abc9c']

def save_fig(fig, name, dpi=100):
    """ä¿å­˜å›¾ç‰‡"""
    fig.savefig(f'figures/{name}.png', dpi=dpi, bbox_inches='tight', facecolor='white')
    print(f"  âœ“ å·²ä¿å­˜: figures/{name}.png")
    plt.close(fig)  # ç«‹å³é‡Šæ”¾å†…å­˜

import gc  # åƒåœ¾å›æ”¶

# ============================================================
# å›¾1: æ¶ˆèå®éªŒ - éªŒè¯å¤šæ¨¡æ€æœ‰æ•ˆæ€§
# ============================================================
print("\nğŸ“Š ç”Ÿæˆå›¾1: æ¶ˆèå®éªŒå¯¹æ¯”...")

fig, ax = plt.subplots(figsize=(8, 5))

experiments = ['Multimodal\n(A1)', 'Text-Only\n(A2)', 'Image-Only\n(A3)']
accuracies = [67.00, 64.75, 62.62]
f1_scores = [0.565, 0.533, 0.344]

x = np.arange(len(experiments))
width = 0.35

bars1 = ax.bar(x - width/2, accuracies, width, label='Accuracy (%)', color='#3498db', edgecolor='black', linewidth=1.2)
bars2 = ax.bar(x + width/2, [f*100 for f in f1_scores], width, label='F1 Score (Ã—100)', color='#2ecc71', edgecolor='black', linewidth=1.2)

ax.set_ylabel('Score', fontsize=12)
ax.set_title('Ablation Study: Multimodal vs Single Modality', fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(experiments, fontsize=11)
ax.legend(loc='upper right', fontsize=10)
ax.set_ylim(0, 80)

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for bar, val in zip(bars1, accuracies):
    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f'{val:.2f}%', 
            ha='center', va='bottom', fontsize=10, fontweight='bold')
for bar, val in zip(bars2, f1_scores):
    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height()*100 + 1, f'{val:.3f}', 
            ha='center', va='bottom', fontsize=10, fontweight='bold')

# æ·»åŠ æå‡æ ‡æ³¨
ax.annotate('', xy=(0, 67), xytext=(1, 64.75),
            arrowprops=dict(arrowstyle='<->', color='red', lw=2))
ax.text(0.5, 66.5, '+2.25%', ha='center', fontsize=10, color='red', fontweight='bold')

ax.annotate('', xy=(0, 67), xytext=(2, 62.62),
            arrowprops=dict(arrowstyle='<->', color='red', lw=2))
ax.text(1, 65, '+4.38%', ha='center', fontsize=10, color='red', fontweight='bold')

plt.tight_layout()
save_fig(fig, 'fig1_ablation_study')
gc.collect()

# ============================================================
# å›¾2: èåˆæ–¹æ³•å¯¹æ¯”
# ============================================================
print("ğŸ“Š ç”Ÿæˆå›¾2: èåˆæ–¹æ³•å¯¹æ¯”...")

fig, ax = plt.subplots(figsize=(10, 5))

fusion_methods = ['Late\nFusion', 'Cross-\nAttention', 'Aligned\nFusion', 'Hierarchical', 'Early\nFusion', 'Gated\nFusion']
fusion_acc = [67.00, 66.75, 66.75, 64.75, 64.12, 61.00]
fusion_colors = ['#2ecc71', '#3498db', '#9b59b6', '#f39c12', '#e74c3c', '#95a5a6']

bars = ax.bar(fusion_methods, fusion_acc, color=fusion_colors, edgecolor='black', linewidth=1.2)

ax.set_ylabel('Validation Accuracy (%)', fontsize=12)
ax.set_title('Comparison of Fusion Methods (Frozen Encoders)', fontsize=14, fontweight='bold')
ax.set_ylim(55, 72)

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for bar, val in zip(bars, fusion_acc):
    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3, f'{val:.2f}%', 
            ha='center', va='bottom', fontsize=10, fontweight='bold')

# æ ‡æ³¨æœ€ä½³
bars[0].set_edgecolor('#27ae60')
bars[0].set_linewidth(3)

ax.axhline(y=67.00, color='green', linestyle='--', alpha=0.5, label='Best: 67.00%')
ax.legend(loc='upper right')

plt.tight_layout()
save_fig(fig, 'fig2_fusion_comparison')
gc.collect()

# ============================================================
# å›¾3: ä¼˜åŒ–é˜¶æ®µæå‡å›¾
# ============================================================
print("ğŸ“Š ç”Ÿæˆå›¾3: ä¼˜åŒ–é˜¶æ®µæå‡...")

fig, ax = plt.subplots(figsize=(9, 5))

stages = ['Baseline\n(Frozen Encoders)', 'Unfrozen Encoders\n+ Layer-wise LR', 'Hyperparameter\nOptimization']
stage_acc = [67.00, 71.25, 72.25]
stage_colors = ['#e74c3c', '#f39c12', '#2ecc71']

bars = ax.bar(stages, stage_acc, color=stage_colors, edgecolor='black', linewidth=1.5, width=0.6)

ax.set_ylabel('Validation Accuracy (%)', fontsize=12)
ax.set_title('Optimization Progress: 67.00% â†’ 72.25% (+5.25%)', fontsize=14, fontweight='bold')
ax.set_ylim(60, 78)

# æ·»åŠ æ•°å€¼æ ‡ç­¾å’Œæå‡
for i, (bar, val) in enumerate(zip(bars, stage_acc)):
    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, f'{val:.2f}%', 
            ha='center', va='bottom', fontsize=12, fontweight='bold')
    if i > 0:
        improvement = val - stage_acc[i-1]
        ax.annotate(f'+{improvement:.2f}%', 
                   xy=(bar.get_x() + bar.get_width()/2, val - 2),
                   fontsize=11, ha='center', color='white', fontweight='bold')

# æ·»åŠ è¿æ¥çº¿
for i in range(len(stages)-1):
    ax.annotate('', xy=(i+1, stage_acc[i+1]-0.5), xytext=(i, stage_acc[i]+0.5),
                arrowprops=dict(arrowstyle='->', color='#34495e', lw=2))

plt.tight_layout()
save_fig(fig, 'fig3_optimization_progress')
gc.collect()

# ============================================================
# å›¾4: æ··æ·†çŸ©é˜µ (Bad Case åˆ†æ)
# ============================================================
print("ğŸ“Š ç”Ÿæˆå›¾4: æ··æ·†çŸ©é˜µ...")

fig, ax = plt.subplots(figsize=(7, 6))

# æ ¹æ® Bad Case åˆ†æç»“æœæ„å»ºæ··æ·†çŸ©é˜µ
# éªŒè¯é›†800æ ·æœ¬ï¼Œå‡†ç¡®ç‡72.25%ï¼Œé”™è¯¯222ä¸ªï¼Œæ­£ç¡®578ä¸ª
# ç±»åˆ«åˆ†å¸ƒï¼špositive 59.7% â‰ˆ 478, negative 29.8% â‰ˆ 238, neutral 10.5% â‰ˆ 84

# é”™è¯¯åˆ†å¸ƒï¼š
# positive â†’ neutral: 65
# neutral â†’ positive: 61  
# negative â†’ positive: 45
# negative â†’ neutral: 18
# positive â†’ negative: 18
# neutral â†’ negative: 15

# æ¨ç®—æ··æ·†çŸ©é˜µ (è¡Œ=çœŸå®, åˆ—=é¢„æµ‹)
# positive: 478 - 65 - 18 = 395 correct
# negative: 238 - 45 - 18 = 175 correct
# neutral: 84 - 61 - 15 = 8 correct

confusion_matrix = np.array([
    [395, 18, 65],   # positive: æ­£ç¡®395, é¢„æµ‹ä¸ºneg 18, é¢„æµ‹ä¸ºneu 65
    [45, 175, 18],   # negative: é¢„æµ‹ä¸ºpos 45, æ­£ç¡®175, é¢„æµ‹ä¸ºneu 18
    [61, 15, 8]      # neutral: é¢„æµ‹ä¸ºpos 61, é¢„æµ‹ä¸ºneg 15, æ­£ç¡®8
])

labels = ['Positive', 'Negative', 'Neutral']

# ç»˜åˆ¶çƒ­åŠ›å›¾
im = ax.imshow(confusion_matrix, cmap='Blues')

# æ·»åŠ é¢œè‰²æ¡
cbar = ax.figure.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
cbar.ax.set_ylabel('Count', rotation=-90, va="bottom", fontsize=11)

# è®¾ç½®åæ ‡è½´
ax.set_xticks(np.arange(len(labels)))
ax.set_yticks(np.arange(len(labels)))
ax.set_xticklabels(labels, fontsize=11)
ax.set_yticklabels(labels, fontsize=11)
ax.set_xlabel('Predicted Label', fontsize=12)
ax.set_ylabel('True Label', fontsize=12)
ax.set_title('Confusion Matrix (HP1_BEST, Val Acc: 72.25%)', fontsize=13, fontweight='bold')

# æ·»åŠ æ•°å€¼
for i in range(len(labels)):
    for j in range(len(labels)):
        val = confusion_matrix[i, j]
        color = 'white' if val > 200 else 'black'
        ax.text(j, i, f'{val}', ha='center', va='center', color=color, fontsize=14, fontweight='bold')

plt.tight_layout()
save_fig(fig, 'fig4_confusion_matrix')
gc.collect()

# ============================================================
# å›¾5: æ•°æ®é¢„å¤„ç†å®éªŒå¯¹æ¯”
# ============================================================
print("ğŸ“Š ç”Ÿæˆå›¾5: æ•°æ®é¢„å¤„ç†å®éªŒ...")

fig, ax = plt.subplots(figsize=(9, 5))

preprocess_methods = ['DA1: Baseline\n(No Preprocessing)', 'DA3: Image\nAugmentation', 
                      'DA4: Text+Image', 'DA2: Text\nCleaning']
preprocess_acc = [71.37, 71.13, 70.63, 70.00]
preprocess_colors = ['#2ecc71', '#3498db', '#9b59b6', '#e74c3c']

bars = ax.bar(preprocess_methods, preprocess_acc, color=preprocess_colors, 
              edgecolor='black', linewidth=1.2, width=0.6)

ax.set_ylabel('Validation Accuracy (%)', fontsize=12)
ax.set_title('Data Preprocessing Experiments', fontsize=14, fontweight='bold')
ax.set_ylim(68, 73)

for bar, val in zip(bars, preprocess_acc):
    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, f'{val:.2f}%', 
            ha='center', va='bottom', fontsize=11, fontweight='bold')

# æ ‡æ³¨æœ€ä½³
bars[0].set_edgecolor('#27ae60')
bars[0].set_linewidth(3)
ax.axhline(y=71.37, color='green', linestyle='--', alpha=0.5)

plt.tight_layout()
save_fig(fig, 'fig5_data_preprocessing')
gc.collect()

# ============================================================
# å›¾6: è¶…å‚æ•°æœç´¢ç»“æœ
# ============================================================
print("ğŸ“Š ç”Ÿæˆå›¾6: è¶…å‚æ•°æœç´¢...")

fig, ax = plt.subplots(figsize=(9, 5))

hp_configs = ['HP1\nDrop=0.2', 'HP4\nDrop=0.3', 'HP5\nDrop=0.25', 'HP2\nDrop=0.3', 'HP3\nDrop=0.3']
hp_acc = [72.25, 71.75, 71.63, 71.50, 70.63]
hp_colors = ['#2ecc71', '#3498db', '#9b59b6', '#f39c12', '#e74c3c']

bars = ax.bar(hp_configs, hp_acc, color=hp_colors, edgecolor='black', linewidth=1.2, width=0.6)

ax.set_ylabel('Validation Accuracy (%)', fontsize=12)
ax.set_title('Hyperparameter Search Results', fontsize=14, fontweight='bold')
ax.set_ylim(69, 74)

for bar, val in zip(bars, hp_acc):
    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, f'{val:.2f}%', 
            ha='center', va='bottom', fontsize=11, fontweight='bold')

# æ ‡æ³¨æœ€ä½³
bars[0].set_edgecolor('#27ae60')
bars[0].set_linewidth(3)
ax.axhline(y=72.25, color='green', linestyle='--', alpha=0.5, label='Best: HP1 (72.25%)')
ax.legend(loc='upper right')

plt.tight_layout()
save_fig(fig, 'fig6_hyperparameter_search')
gc.collect()

# ============================================================
# å›¾7: é”™è¯¯ç±»å‹åˆ†å¸ƒé¥¼å›¾
# ============================================================
print("ğŸ“Š ç”Ÿæˆå›¾7: é”™è¯¯ç±»å‹åˆ†å¸ƒ...")

fig, ax = plt.subplots(figsize=(8, 8))

error_types = ['posâ†’neu\n(65)', 'neuâ†’pos\n(61)', 'negâ†’pos\n(45)', 
               'negâ†’neu\n(18)', 'posâ†’neg\n(18)', 'neuâ†’neg\n(15)']
error_counts = [65, 61, 45, 18, 18, 15]
error_colors = ['#e74c3c', '#c0392b', '#3498db', '#2980b9', '#f39c12', '#d35400']

explode = (0.05, 0.05, 0, 0, 0, 0)  # çªå‡ºå‰ä¸¤ä¸ªæœ€å¤§çš„é”™è¯¯ç±»å‹

wedges, texts, autotexts = ax.pie(error_counts, explode=explode, labels=error_types, 
                                   colors=error_colors, autopct='%1.1f%%',
                                   shadow=True, startangle=90,
                                   textprops={'fontsize': 11})

for autotext in autotexts:
    autotext.set_fontweight('bold')
    autotext.set_fontsize(10)

ax.set_title('Error Type Distribution (222 errors total)\nMain Issue: posâ†”neu confusion (56.8%)', 
             fontsize=13, fontweight='bold')

plt.tight_layout()
save_fig(fig, 'fig7_error_distribution')
gc.collect()

# ============================================================
# å›¾8: æ¨¡å‹æ¶æ„ç¤ºæ„å›¾ (ç®€åŒ–ç‰ˆ)
# ============================================================
print("ğŸ“Š ç”Ÿæˆå›¾8: æ¨¡å‹æ¶æ„...")

fig, ax = plt.subplots(figsize=(12, 8))
ax.set_xlim(0, 12)
ax.set_ylim(0, 10)
ax.axis('off')

# ç»˜åˆ¶æ–¹æ¡†å‡½æ•°
def draw_box(ax, x, y, w, h, text, color='#3498db', fontsize=10):
    rect = plt.Rectangle((x, y), w, h, facecolor=color, edgecolor='black', linewidth=2, alpha=0.8)
    ax.add_patch(rect)
    ax.text(x + w/2, y + h/2, text, ha='center', va='center', fontsize=fontsize, fontweight='bold', wrap=True)

# è¾“å…¥å±‚
draw_box(ax, 0.5, 8, 2.5, 1.2, 'Text Input', '#ecf0f1', 11)
draw_box(ax, 9, 8, 2.5, 1.2, 'Image Input', '#ecf0f1', 11)

# ç¼–ç å™¨
draw_box(ax, 0.5, 5.5, 2.5, 1.8, 'DistilBERT\n(Unfrozen\nlast 2 layers)', '#3498db', 10)
draw_box(ax, 9, 5.5, 2.5, 1.8, 'ResNet50\n(Unfrozen\nlayer4)', '#e74c3c', 10)

# æŠ•å½±å±‚
draw_box(ax, 0.5, 4, 2.5, 1, '768 â†’ 512', '#95a5a6', 10)
draw_box(ax, 9, 4, 2.5, 1, '2048 â†’ 512', '#95a5a6', 10)

# Cross-Attention Fusion
draw_box(ax, 4, 2.5, 4, 1.5, 'Cross-Attention Fusion\nQ=Text, K/V=Image | Q=Image, K/V=Text', '#2ecc71', 10)

# åˆ†ç±»å™¨
draw_box(ax, 4.5, 0.5, 3, 1.2, 'Classifier\n512 â†’ 3', '#9b59b6', 11)

# ç®­å¤´
arrow_props = dict(arrowstyle='->', color='#34495e', lw=2)
ax.annotate('', xy=(1.75, 7.3), xytext=(1.75, 8), arrowprops=arrow_props)
ax.annotate('', xy=(10.25, 7.3), xytext=(10.25, 8), arrowprops=arrow_props)
ax.annotate('', xy=(1.75, 5), xytext=(1.75, 5.5), arrowprops=arrow_props)
ax.annotate('', xy=(10.25, 5), xytext=(10.25, 5.5), arrowprops=arrow_props)
ax.annotate('', xy=(4, 3.25), xytext=(3, 4.25), arrowprops=arrow_props)
ax.annotate('', xy=(8, 3.25), xytext=(9, 4.25), arrowprops=arrow_props)
ax.annotate('', xy=(6, 1.7), xytext=(6, 2.5), arrowprops=arrow_props)

# å­¦ä¹ ç‡æ ‡æ³¨
ax.text(3.2, 6.3, 'LR: 1e-5', fontsize=9, color='#2980b9', style='italic')
ax.text(11.7, 6.3, 'LR: 1e-5', fontsize=9, color='#c0392b', style='italic')
ax.text(6, 2.1, 'LR: 5e-5', fontsize=9, color='#27ae60', style='italic')
ax.text(6, 0.2, 'LR: 1e-4', fontsize=9, color='#8e44ad', style='italic')

ax.set_title('Model Architecture: Cross-Attention Fusion with Layer-wise Learning Rates', 
             fontsize=14, fontweight='bold', pad=20)

plt.tight_layout()
save_fig(fig, 'fig8_model_architecture')
gc.collect()

# ============================================================
# å›¾9: ç±»åˆ«åˆ†å¸ƒå›¾
# ============================================================
print("ğŸ“Š ç”Ÿæˆå›¾9: æ•°æ®ç±»åˆ«åˆ†å¸ƒ...")

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ
labels = ['Positive', 'Negative', 'Neutral']
sizes = [2388, 1193, 419]
colors_pie = ['#2ecc71', '#e74c3c', '#f39c12']
explode = (0.02, 0.02, 0.05)

axes[0].pie(sizes, explode=explode, labels=labels, colors=colors_pie, autopct='%1.1f%%',
            shadow=True, startangle=90, textprops={'fontsize': 11, 'fontweight': 'bold'})
axes[0].set_title('Training Data Distribution\n(4000 samples)', fontsize=13, fontweight='bold')

# æŸ±çŠ¶å›¾æ˜¾ç¤ºæ•°é‡
bars = axes[1].bar(labels, sizes, color=colors_pie, edgecolor='black', linewidth=1.5)
axes[1].set_ylabel('Number of Samples', fontsize=12)
axes[1].set_title('Class Imbalance Analysis', fontsize=13, fontweight='bold')

for bar, val in zip(bars, sizes):
    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 30, str(val), 
                ha='center', va='bottom', fontsize=12, fontweight='bold')

# æ·»åŠ ä¸å¹³è¡¡æ¯”ä¾‹æ ‡æ³¨
axes[1].axhline(y=1333, color='gray', linestyle='--', alpha=0.5, label='Balanced: 1333')
axes[1].legend(loc='upper right')

plt.tight_layout()
save_fig(fig, 'fig9_class_distribution')
gc.collect()

# ============================================================
# æ±‡æ€»
# ============================================================
print("\n" + "="*60)
print("âœ… æ‰€æœ‰å›¾è¡¨å·²ç”Ÿæˆå®Œæˆï¼ä¿å­˜åœ¨ figures/ ç›®å½•")
print("="*60)
print("\nğŸ“ ç”Ÿæˆçš„å›¾è¡¨åˆ—è¡¨ï¼š")
print("   1. fig1_ablation_study      - æ¶ˆèå®éªŒï¼ˆè¯æ˜å¤šæ¨¡æ€æœ‰æ•ˆæ€§ï¼‰â­")
print("   2. fig2_fusion_comparison   - èåˆæ–¹æ³•å¯¹æ¯”")
print("   3. fig3_optimization_progress - ä¼˜åŒ–é˜¶æ®µæå‡ â­")
print("   4. fig4_confusion_matrix    - æ··æ·†çŸ©é˜µï¼ˆBad Caseåˆ†æï¼‰â­")
print("   5. fig5_data_preprocessing  - æ•°æ®é¢„å¤„ç†å®éªŒ")
print("   6. fig6_hyperparameter_search - è¶…å‚æ•°æœç´¢ç»“æœ")
print("   7. fig7_error_distribution  - é”™è¯¯ç±»å‹åˆ†å¸ƒ")
print("   8. fig8_model_architecture  - æ¨¡å‹æ¶æ„å›¾ â­")
print("   9. fig9_class_distribution  - æ•°æ®ç±»åˆ«åˆ†å¸ƒ")
print("\nâ­ æ ‡è®°ä¸ºæ¨èæ”¾å…¥å®éªŒæŠ¥å‘Šçš„é‡è¦å›¾è¡¨")
print("\nğŸ“ å®éªŒæŠ¥å‘Šå»ºè®®ä½¿ç”¨çš„å›¾è¡¨ï¼š")
print("   - å›¾1 (æ¶ˆèå®éªŒ): è¯æ˜å¤šæ¨¡æ€èåˆçš„æœ‰æ•ˆæ€§")
print("   - å›¾3 (ä¼˜åŒ–æå‡): å±•ç¤ºä»67%åˆ°72.25%çš„ä¼˜åŒ–å†ç¨‹")  
print("   - å›¾4 (æ··æ·†çŸ©é˜µ): Bad Caseåˆ†æï¼Œå±•ç¤ºpos/neuæ··æ·†é—®é¢˜")
print("   - å›¾8 (æ¨¡å‹æ¶æ„): æ¸…æ™°å±•ç¤ºæ•´ä½“æ¶æ„è®¾è®¡")
