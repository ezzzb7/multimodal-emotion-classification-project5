"""
åŸºäºæ•°æ®é©±åŠ¨çš„åŠ¡å®ä¼˜åŒ–æ–¹æ¡ˆ

æ ¸å¿ƒç­–ç•¥ï¼š
1. âœ… ä¸è¿½æ±‚è¿‡äºå¤æ‚çš„æŠ€å·§ï¼ˆè¿‡é‡‡æ ·/Mixupå·²éªŒè¯æ— æ•ˆï¼‰
2. âœ… ä¸“æ³¨äºæå‡å½“å‰æœ€ä½³æ¨¡å‹ (OPT_cross_attention 71.25%)
3. âœ… é€šè¿‡è¶…å‚æ•°å¾®è°ƒæ¦¨å–æœ€åçš„æ€§èƒ½
4. âœ… æ¨¡å‹é›†æˆæå‡é²æ£’æ€§
5. âœ… æ”¯æŒæ–­ç‚¹ç»­ä¼ 
"""
import os
import sys
import json
import torch
import torch.nn as nn
import numpy as np
from transformers import DistilBertTokenizer
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup
from tqdm import tqdm

sys.path.insert(0, os.path.dirname(__file__))

from run_experiment_optimized import OptimizedMultimodalClassifier, OPTIMIZED_CONFIG
from data.data_loader import get_data_loaders
from utils.train_utils import set_seed, compute_metrics, EarlyStopping


# ========== æ–­ç‚¹ç»­ä¼ æ–‡ä»¶ ==========
CHECKPOINT_FILE = 'experiments/hyperparam_checkpoint.json'
ENSEMBLE_CHECKPOINT_FILE = 'experiments/ensemble_checkpoint.json'


def load_checkpoint(checkpoint_file):
    """åŠ è½½æ–­ç‚¹"""
    if os.path.exists(checkpoint_file):
        with open(checkpoint_file, 'r') as f:
            return json.load(f)
    return {'completed': [], 'results': []}


def save_checkpoint(checkpoint_file, data):
    """ä¿å­˜æ–­ç‚¹"""
    os.makedirs(os.path.dirname(checkpoint_file), exist_ok=True)
    with open(checkpoint_file, 'w') as f:
        json.dump(data, f, indent=2)


# ========== è¶…å‚æ•°ç½‘æ ¼æœç´¢ ==========
HYPERPARAM_GRID = {
    'dropout': [0.2, 0.3, 0.4],  # å½“å‰0.3ï¼Œå°è¯•æ›´å°/æ›´å¤§
    'lr_classifier': [5e-5, 1e-4, 2e-4],  # å½“å‰1e-4
    'weight_decay': [0.005, 0.01, 0.02],  # å½“å‰0.01
}


def train_with_hyperparams(exp_id, dropout, lr_classifier, weight_decay):
    """ä½¿ç”¨æŒ‡å®šè¶…å‚æ•°è®­ç»ƒæ¨¡å‹"""
    
    config = OPTIMIZED_CONFIG.copy()
    config['dropout'] = dropout
    config['lr_classifier'] = lr_classifier
    config['weight_decay'] = weight_decay
    
    set_seed(config['seed'])
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"è®¾å¤‡: {device}")
    
    # ç¦»çº¿åŠ è½½tokenizerï¼Œé¿å…ç½‘ç»œé—®é¢˜
    print("åŠ è½½ Tokenizer...")
    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', local_files_only=True)
    print("  âœ“ Tokenizer åŠ è½½å®Œæˆ")
    
    # åŠ è½½æ•°æ®
    print("åŠ è½½æ•°æ®...")
    train_loader, val_loader, _ = get_data_loaders(
        data_dir=config['data_dir'],
        train_label_file=config['train_label'],
        batch_size=config['batch_size'],
        val_ratio=config['val_ratio'],
        num_workers=0,
        seed=config['seed'],
        force_resplit=True
    )
    print(f"  âœ“ æ•°æ®åŠ è½½å®Œæˆ: è®­ç»ƒé›† {len(train_loader.dataset)}, éªŒè¯é›† {len(val_loader.dataset)}")
    
    # åˆ›å»ºæ¨¡å‹
    print("åˆ›å»ºæ¨¡å‹...")
    model = OptimizedMultimodalClassifier(
        num_classes=3,
        feature_dim=config['feature_dim'],
        fusion_type='cross_attention',  # ä½¿ç”¨æœ€ä½³èåˆæ–¹æ³•
        dropout=dropout,
        unfreeze_text_layers=config['unfreeze_text_layers'],
        unfreeze_image_layers=config['unfreeze_image_layers']
    ).to(device)
    print("  âœ“ æ¨¡å‹åˆ›å»ºå®Œæˆ")
    
    # ç»Ÿè®¡å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"  å‚æ•°é‡: {trainable_params:,} / {total_params:,}")
    
    # è®­ç»ƒè®¾ç½®
    print("è®¾ç½®ä¼˜åŒ–å™¨...")
    class_weights = torch.FloatTensor([1.0, 1.5, 3.0]).to(device)
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    
    param_groups = model.get_param_groups(
        lr_pretrained=config['lr_pretrained'],
        lr_fusion=config['lr_fusion'],
        lr_classifier=lr_classifier
    )
    optimizer = AdamW(param_groups, weight_decay=weight_decay)
    
    total_steps = len(train_loader) * config['num_epochs'] // config['accumulation_steps']
    warmup_steps = int(total_steps * config['warmup_ratio'])
    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)
    
    early_stopping = EarlyStopping(patience=config['early_stopping_patience'], mode='max')
    print("  âœ“ ä¼˜åŒ–å™¨è®¾ç½®å®Œæˆ")
    
    # å¿«é€Ÿè®­ç»ƒï¼ˆå‡å°‘epochï¼‰
    print("\nå¼€å§‹è®­ç»ƒ...")
    import sys
    sys.stdout.flush()
    
    best_val_acc = 0
    best_val_f1 = 0
    best_epoch = 0
    
    for epoch in range(15):  # å‡å°‘åˆ°15ä¸ªepochå¿«é€ŸéªŒè¯
        # è®­ç»ƒ
        model.train()
        batch_count = 0
        print(f"  Epoch {epoch+1}/15 [", end="", flush=True)
        
        for step, batch in enumerate(train_loader):
            texts = batch['text']
            images = batch['image'].to(device)
            labels = batch['label'].to(device)
            
            encoded = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors='pt')
            input_ids = encoded['input_ids'].to(device)
            attention_mask = encoded['attention_mask'].to(device)
            
            logits = model(input_ids, attention_mask, images)
            loss = criterion(logits, labels) / config['accumulation_steps']
            loss.backward()
            
            if (step + 1) % config['accumulation_steps'] == 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), config['max_grad_norm'])
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()
            
            batch_count += 1
            # æ¯20ä¸ªbatchæ‰“å°ä¸€ä¸ªç‚¹ (å…±400ä¸ªbatchï¼Œæ˜¾ç¤º20ä¸ªç‚¹)
            if batch_count % 20 == 0:
                print(".", end="", flush=True)
        
        print("] ", end="", flush=True)
        
        # éªŒè¯
        model.eval()
        val_preds, val_labels = [], []
        
        with torch.no_grad():
            for batch in val_loader:
                texts = batch['text']
                images = batch['image'].to(device)
                labels = batch['label'].to(device)
                
                encoded = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors='pt')
                input_ids = encoded['input_ids'].to(device)
                attention_mask = encoded['attention_mask'].to(device)
                
                logits = model(input_ids, attention_mask, images)
                val_preds.extend(torch.argmax(logits, dim=1).cpu().numpy())
                val_labels.extend(labels.cpu().numpy())
        
        val_metrics = compute_metrics(val_preds, val_labels)
        print(f"Val Acc: {val_metrics['accuracy']:.4f}, F1: {val_metrics['f1']:.4f}", flush=True)
        
        if val_metrics['accuracy'] > best_val_acc:
            best_val_acc = val_metrics['accuracy']
            best_val_f1 = val_metrics['f1']
            best_epoch = epoch + 1
            print(f"    âœ“ æ–°æœ€ä½³!", flush=True)
        
        if early_stopping(val_metrics['accuracy'], epoch):
            print(f"  Early stopping at epoch {epoch+1}")
            break
    
    return {
        'exp_id': exp_id,
        'dropout': dropout,
        'lr_classifier': lr_classifier,
        'weight_decay': weight_decay,
        'val_acc': best_val_acc,
        'val_f1': best_val_f1,
        'best_epoch': best_epoch
    }


def hyperparam_search():
    """è¶…å‚æ•°ç½‘æ ¼æœç´¢ï¼ˆå¿«é€Ÿç‰ˆï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰"""
    
    print("\n" + "="*70)
    print("è¶…å‚æ•°ç½‘æ ¼æœç´¢ (æ”¯æŒæ–­ç‚¹ç»­ä¼ )")
    print("="*70)
    
    # åŠ è½½æ–­ç‚¹
    checkpoint = load_checkpoint(CHECKPOINT_FILE)
    completed = set(checkpoint.get('completed', []))
    results = checkpoint.get('results', [])
    
    if completed:
        print(f"âœ“ ä»æ–­ç‚¹æ¢å¤ï¼Œå·²å®Œæˆ {len(completed)} ä¸ªå®éªŒ")
    
    # åªæµ‹è¯•å…³é”®ç»„åˆ
    important_configs = [
        # (dropout, lr_classifier, weight_decay, exp_id)
        (0.2, 1e-4, 0.01, 'HP1'),    # å‡å°‘dropout
        (0.3, 1e-4, 0.02, 'HP2'),    # å¢åŠ æ­£åˆ™åŒ–
        (0.3, 5e-5, 0.01, 'HP3'),    # å‡å°å­¦ä¹ ç‡
        (0.3, 2e-4, 0.01, 'HP4'),    # å¢å¤§å­¦ä¹ ç‡
        (0.25, 1e-4, 0.015, 'HP5'),  # å¹³è¡¡é…ç½®
    ]
    
    for dropout, lr_clf, wd, exp_id in important_configs:
        # è·³è¿‡å·²å®Œæˆçš„å®éªŒ
        if exp_id in completed:
            print(f"\n[{exp_id}] å·²å®Œæˆï¼Œè·³è¿‡")
            continue
        
        print(f"\n{'='*50}")
        print(f"[{exp_id}] æµ‹è¯•é…ç½®:")
        print(f"  Dropout: {dropout}")
        print(f"  LR_Classifier: {lr_clf}")
        print(f"  Weight_Decay: {wd}")
        print(f"{'='*50}")
        
        try:
            result = train_with_hyperparams(exp_id, dropout, lr_clf, wd)
            results.append(result)
            completed.add(exp_id)
            
            # ä¿å­˜æ–­ç‚¹
            save_checkpoint(CHECKPOINT_FILE, {
                'completed': list(completed),
                'results': results
            })
            
            print(f"  âœ“ Val Acc: {result['val_acc']:.4f}, Val F1: {result['val_f1']:.4f}")
            print(f"  âœ“ æ–­ç‚¹å·²ä¿å­˜")
            
        except Exception as e:
            print(f"  âŒ å®éªŒå¤±è´¥: {e}")
            # ä¿å­˜å½“å‰è¿›åº¦
            save_checkpoint(CHECKPOINT_FILE, {
                'completed': list(completed),
                'results': results
            })
            raise
    
    # æ‰“å°ç»“æœ
    print("\n" + "="*70)
    print("è¶…å‚æ•°æœç´¢ç»“æœ")
    print("="*70)
    print(f"{'ID':<8} {'Dropout':<10} {'LR_Clf':<12} {'WD':<10} {'Val Acc':<10} {'Val F1':<10}")
    print("-"*70)
    
    for r in sorted(results, key=lambda x: -x['val_acc']):
        print(f"{r['exp_id']:<8} {r['dropout']:<10} {r['lr_classifier']:<12} {r['weight_decay']:<10} {r['val_acc']:.4f}     {r['val_f1']:.4f}")
    
    if results:
        best = max(results, key=lambda x: x['val_acc'])
        print(f"\nğŸ† æœ€ä½³é…ç½®:")
        print(f"   Dropout: {best['dropout']}")
        print(f"   LR_Classifier: {best['lr_classifier']}")
        print(f"   Weight_Decay: {best['weight_decay']}")
        print(f"   Val Acc: {best['val_acc']:.4f}")
    
    return results


def train_ensemble_models():
    """è®­ç»ƒé›†æˆæ¨¡å‹ï¼ˆå¤šä¸ªç§å­ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰"""
    
    print("\n" + "="*70)
    print("è®­ç»ƒé›†æˆæ¨¡å‹ (æ”¯æŒæ–­ç‚¹ç»­ä¼ )")
    print("="*70)
    
    # åŠ è½½æ–­ç‚¹
    checkpoint = load_checkpoint(ENSEMBLE_CHECKPOINT_FILE)
    completed_seeds = set(checkpoint.get('completed_seeds', []))
    models = checkpoint.get('models', [])
    
    if completed_seeds:
        print(f"âœ“ ä»æ–­ç‚¹æ¢å¤ï¼Œå·²å®Œæˆ {len(completed_seeds)} ä¸ªæ¨¡å‹")
    
    seeds = [42, 123, 456]  # 3ä¸ªä¸åŒç§å­
    
    for i, seed in enumerate(seeds):
        # è·³è¿‡å·²å®Œæˆçš„
        if seed in completed_seeds:
            print(f"\n[{i+1}/{len(seeds)}] Seed={seed} å·²å®Œæˆï¼Œè·³è¿‡")
            continue
        
        print(f"\n[{i+1}/{len(seeds)}] è®­ç»ƒæ¨¡å‹ (seed={seed})")
        
        config = OPTIMIZED_CONFIG.copy()
        config['seed'] = seed
        set_seed(seed)
        
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
        
        train_loader, val_loader, _ = get_data_loaders(
            data_dir=config['data_dir'],
            train_label_file=config['train_label'],
            batch_size=config['batch_size'],
            val_ratio=config['val_ratio'],
            num_workers=0,
            seed=seed,
            force_resplit=True
        )
        
        model = OptimizedMultimodalClassifier(
            num_classes=3,
            feature_dim=config['feature_dim'],
            fusion_type='cross_attention',
            dropout=config['dropout'],
            unfreeze_text_layers=config['unfreeze_text_layers'],
            unfreeze_image_layers=config['unfreeze_image_layers']
        ).to(device)
        
        # è®­ç»ƒï¼ˆç®€åŒ–ç‰ˆï¼‰
        class_weights = torch.FloatTensor([1.0, 1.5, 3.0]).to(device)
        criterion = nn.CrossEntropyLoss(weight=class_weights)
        
        param_groups = model.get_param_groups(
            lr_pretrained=config['lr_pretrained'],
            lr_fusion=config['lr_fusion'],
            lr_classifier=config['lr_classifier']
        )
        optimizer = AdamW(param_groups, weight_decay=config['weight_decay'])
        
        total_steps = len(train_loader) * 20 // config['accumulation_steps']
        warmup_steps = int(total_steps * config['warmup_ratio'])
        scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)
        
        early_stopping = EarlyStopping(patience=7, mode='max')
        
        best_val_acc = 0
        
        for epoch in range(20):
            # è®­ç»ƒ
            model.train()
            for step, batch in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}', leave=False)):
                texts = batch['text']
                images = batch['image'].to(device)
                labels = batch['label'].to(device)
                
                encoded = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors='pt')
                input_ids = encoded['input_ids'].to(device)
                attention_mask = encoded['attention_mask'].to(device)
                
                logits = model(input_ids, attention_mask, images)
                loss = criterion(logits, labels) / config['accumulation_steps']
                loss.backward()
                
                if (step + 1) % config['accumulation_steps'] == 0:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), config['max_grad_norm'])
                    optimizer.step()
                    scheduler.step()
                    optimizer.zero_grad()
            
            # éªŒè¯
            model.eval()
            val_preds, val_labels = [], []
            
            with torch.no_grad():
                for batch in val_loader:
                    texts = batch['text']
                    images = batch['image'].to(device)
                    labels = batch['label'].to(device)
                    
                    encoded = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors='pt')
                    input_ids = encoded['input_ids'].to(device)
                    attention_mask = encoded['attention_mask'].to(device)
                    
                    logits = model(input_ids, attention_mask, images)
                    val_preds.extend(torch.argmax(logits, dim=1).cpu().numpy())
                    val_labels.extend(labels.cpu().numpy())
            
            val_metrics = compute_metrics(val_preds, val_labels)
            print(f"  Epoch {epoch+1}: Val Acc={val_metrics['accuracy']:.4f}")
            
            if val_metrics['accuracy'] > best_val_acc:
                best_val_acc = val_metrics['accuracy']
                torch.save(model.state_dict(), f'experiments/checkpoints/ENSEMBLE_{i+1}_seed{seed}_best.pth')
                print(f"    âœ“ æ–°æœ€ä½³! Val Acc={best_val_acc:.4f}")
            
            if early_stopping(val_metrics['accuracy'], epoch):
                print(f"  Early stopping at epoch {epoch+1}")
                break
        
        models.append({
            'seed': seed,
            'val_acc': best_val_acc,
            'model_path': f'experiments/checkpoints/ENSEMBLE_{i+1}_seed{seed}_best.pth'
        })
        completed_seeds.add(seed)
        
        # ä¿å­˜æ–­ç‚¹
        save_checkpoint(ENSEMBLE_CHECKPOINT_FILE, {
            'completed_seeds': list(completed_seeds),
            'models': models
        })
        print(f"  âœ“ æ–­ç‚¹å·²ä¿å­˜")
    
    print("\n" + "="*70)
    print("é›†æˆæ¨¡å‹è®­ç»ƒå®Œæˆ")
    print("="*70)
    for m in models:
        print(f"  Seed {m['seed']}: Val Acc={m['val_acc']:.4f}")
    
    if models:
        avg_acc = np.mean([m['val_acc'] for m in models])
        print(f"\nå¹³å‡å‡†ç¡®ç‡: {avg_acc:.4f}")
    
    return models


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='åŠ¡å®ä¼˜åŒ–æ–¹æ¡ˆ')
    parser.add_argument('--hyperparam', action='store_true', help='è¶…å‚æ•°æœç´¢')
    parser.add_argument('--ensemble', action='store_true', help='è®­ç»ƒé›†æˆæ¨¡å‹')
    
    args = parser.parse_args()
    
    if args.hyperparam:
        hyperparam_search()
    elif args.ensemble:
        train_ensemble_models()
    else:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  py -3.11 run_practical_optimization.py --hyperparam  # è¶…å‚æ•°æœç´¢")
        print("  py -3.11 run_practical_optimization.py --ensemble    # è®­ç»ƒé›†æˆæ¨¡å‹")


if __name__ == '__main__':
    main()
