"""
Bad Case åˆ†æå·¥å…·
æ‰¾å‡ºæ¨¡å‹é¢„æµ‹é”™è¯¯çš„æ ·æœ¬ï¼Œåˆ†æå…±åŒç‰¹å¾ï¼Œä¸ºé’ˆå¯¹æ€§ä¼˜åŒ–æä¾›ä¾æ®

âš ï¸ é‡è¦ï¼šåªåˆ†æè®­ç»ƒé›†ï¼Œé¿å…éªŒè¯é›†ä¿¡æ¯æ³„éœ²
   - éªŒè¯é›†ç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œä¸èƒ½å‚ä¸è®­ç»ƒæ•°æ®çš„é€‰æ‹©
   - æ•°æ®å¢å¼ºåªèƒ½åŸºäºè®­ç»ƒé›†çš„åˆ†æç»“æœ
"""
import torch
import numpy as np
import pandas as pd
from collections import Counter, defaultdict
import os
import sys
import argparse

sys.path.insert(0, os.path.dirname(__file__))

from models.multimodal_model import MultimodalClassifier
from data.data_loader import get_data_loaders
from utils.train_utils import load_checkpoint


def analyze_bad_cases(model, dataloader, device='cpu', label_map=None):
    """
    åˆ†ææ¨¡å‹çš„é”™è¯¯é¢„æµ‹
    
    Returns:
        bad_cases: list of dicts with error info
        stats: dict with statistics
    """
    model.eval()
    bad_cases = []
    
    label_names = {0: 'positive', 1: 'negative', 2: 'neutral'}
    if label_map:
        label_names = {v: k for k, v in label_map.items()}
    
    with torch.no_grad():
        for batch in dataloader:
            texts = batch['text']
            images = batch['image'].to(device)
            labels = batch['label'].to(device)
            guids = batch['guid']
            
            outputs = model(texts, images)
            preds = torch.argmax(outputs, dim=1)
            probs = torch.softmax(outputs, dim=1)
            
            # æ‰¾å‡ºé”™è¯¯é¢„æµ‹ï¼ˆåªä¿ç•™é«˜ç½®ä¿¡åº¦é”™è¯¯ï¼Œé¿å…å™ªå£°æ ·æœ¬ï¼‰
            for i in range(len(labels)):
                if preds[i] != labels[i]:
                    # ç¡®ä¿GUIDæ˜¯å­—ç¬¦ä¸²æˆ–æ•´æ•°ï¼Œè€Œä¸æ˜¯tensor
                    guid = guids[i]
                    if isinstance(guid, torch.Tensor):
                        guid = guid.item()
                    
                    # è®¡ç®—é¢„æµ‹ç½®ä¿¡åº¦
                    pred_confidence = probs[i][preds[i]].item()
                    
                    # åªä¿ç•™é«˜ç½®ä¿¡åº¦é”™è¯¯ï¼ˆ>0.7ï¼‰ï¼Œè¿™äº›æ›´å¯èƒ½æ˜¯çœŸæ­£çš„å›°éš¾æ ·æœ¬
                    if pred_confidence > 0.7:
                        bad_case = {
                            'guid': guid,
                            'text': texts[i][:200],  # æˆªå–å‰200å­—ç¬¦
                            'true_label': label_names[labels[i].item()],
                            'pred_label': label_names[preds[i].item()],
                            'confidence': pred_confidence,
                            'true_prob': probs[i][labels[i]].item(),
                            'probs': probs[i].cpu().numpy()
                        }
                        bad_cases.append(bad_case)
    
    # ç»Ÿè®¡åˆ†æ
    stats = analyze_error_patterns(bad_cases)
    
    return bad_cases, stats


def analyze_error_patterns(bad_cases):
    """åˆ†æé”™è¯¯æ¨¡å¼"""
    stats = {
        'total_errors': len(bad_cases),
        'confusion_matrix': defaultdict(int),
        'low_confidence_errors': 0,
        'high_confidence_errors': 0,
        'avg_confidence': 0,
        'text_length_stats': []
    }
    
    if not bad_cases:
        return stats
    
    confidences = []
    text_lengths = []
    
    for case in bad_cases:
        # æ··æ·†çŸ©é˜µ
        key = f"{case['true_label']} â†’ {case['pred_label']}"
        stats['confusion_matrix'][key] += 1
        
        # ç½®ä¿¡åº¦ç»Ÿè®¡
        conf = case['confidence']
        confidences.append(conf)
        
        if conf < 0.5:
            stats['low_confidence_errors'] += 1
        else:
            stats['high_confidence_errors'] += 1
        
        # æ–‡æœ¬é•¿åº¦
        text_lengths.append(len(case['text']))
    
    stats['avg_confidence'] = np.mean(confidences)
    stats['text_length_stats'] = {
        'mean': np.mean(text_lengths),
        'std': np.std(text_lengths),
        'min': np.min(text_lengths),
        'max': np.max(text_lengths)
    }
    
    return stats


def save_bad_cases(bad_cases, output_path='bad_cases_analysis.csv'):
    """ä¿å­˜bad casesåˆ°CSV"""
    if not bad_cases:
        print("No bad cases found!")
        return
    
    df = pd.DataFrame(bad_cases)
    df.to_csv(output_path, index=False, encoding='utf-8')
    print(f"âœ“ Bad cases saved to: {output_path}")


def print_analysis_report(stats, bad_cases):
    """æ‰“å°åˆ†ææŠ¥å‘Š"""
    print("\n" + "="*70)
    print("Bad Case Analysis Report")
    print("="*70)
    
    print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    print(f"  - é”™è¯¯æ ·æœ¬æ•°: {stats['total_errors']}")
    print(f"  - å¹³å‡é¢„æµ‹ç½®ä¿¡åº¦: {stats['avg_confidence']:.4f}")
    print(f"  - ä½ç½®ä¿¡åº¦é”™è¯¯ (<0.5): {stats['low_confidence_errors']}")
    print(f"  - é«˜ç½®ä¿¡åº¦é”™è¯¯ (â‰¥0.5): {stats['high_confidence_errors']}")
    
    print(f"\nğŸ”€ æ··æ·†çŸ©é˜µ (é”™è¯¯ç±»å‹åˆ†å¸ƒ):")
    for error_type, count in sorted(stats['confusion_matrix'].items(), key=lambda x: -x[1])[:10]:
        percentage = count / stats['total_errors'] * 100
        print(f"  {error_type}: {count} ({percentage:.1f}%)")
    
    print(f"\nğŸ“ æ–‡æœ¬é•¿åº¦ç»Ÿè®¡:")
    tl = stats['text_length_stats']
    print(f"  - å¹³å‡é•¿åº¦: {tl['mean']:.1f} Â± {tl['std']:.1f}")
    print(f"  - èŒƒå›´: [{tl['min']}, {tl['max']}]")
    
    # é«˜ç½®ä¿¡åº¦é”™è¯¯æ¡ˆä¾‹ï¼ˆæœ€å€¼å¾—å…³æ³¨ï¼‰
    print(f"\nâš ï¸ é«˜ç½®ä¿¡åº¦é”™è¯¯æ¡ˆä¾‹ (Top 5):")
    high_conf_errors = sorted([bc for bc in bad_cases if bc['confidence'] >= 0.5],
                             key=lambda x: -x['confidence'])[:5]
    for i, case in enumerate(high_conf_errors, 1):
        print(f"\n  {i}. GUID: {case['guid']}")
        print(f"     çœŸå®: {case['true_label']} | é¢„æµ‹: {case['pred_label']} (ç½®ä¿¡åº¦: {case['confidence']:.3f})")
        print(f"     æ–‡æœ¬: {case['text'][:100]}...")
    
    print("\n" + "="*70)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Bad Caseåˆ†æå·¥å…·')
    parser.add_argument('--split', type=str, default='train', choices=['train', 'val'],
                        help='åˆ†æå“ªä¸ªæ•°æ®é›† (é»˜è®¤: train, æ¨èåªç”¨trainé¿å…ä¿¡æ¯æ³„éœ²)')
    parser.add_argument('--checkpoint', type=str, 
                        default='checkpoints/best_early_multimodal_20260120_195503.pth',
                        help='æ¨¡å‹checkpointè·¯å¾„')
    args = parser.parse_args()
    
    # é…ç½®
    checkpoint_path = args.checkpoint
    data_dir = r'D:\å½“ä»£äººå·¥æ™ºèƒ½\project5\data'
    train_file = r'D:\å½“ä»£äººå·¥æ™ºèƒ½\project5\train.txt'
    device = 'cpu'
    
    print("="*70)
    print("Bad Case Analysis Tool")
    print("="*70)
    print(f"\nâš ï¸ åˆ†ææ•°æ®é›†: {args.split.upper()}")
    
    if args.split == 'val':
        print("   è­¦å‘Š: åˆ†æéªŒè¯é›†å¯èƒ½å¯¼è‡´ä¿¡æ¯æ³„éœ²ï¼Œå»ºè®®ä½¿ç”¨ --split train")
    else:
        print("   âœ“ æ­£ç¡®: åªåˆ†æè®­ç»ƒé›†ï¼Œé¿å…éªŒè¯é›†ä¿¡æ¯æ³„éœ²")
    
    # åŠ è½½æ•°æ®
    print("\nåŠ è½½æ•°æ®...")
    train_loader, val_loader, _ = get_data_loaders(
        data_dir=data_dir,
        train_label_file=train_file,
        batch_size=8,
        val_ratio=0.15,
        num_workers=0,
        seed=42
    )
    
    # é€‰æ‹©æ•°æ®é›†
    target_loader = train_loader if args.split == 'train' else val_loader
    
    # åˆ›å»ºæ¨¡å‹
    print("åˆ›å»ºæ¨¡å‹...")
    model = MultimodalClassifier(
        num_classes=3,
        text_model='distilbert-base-uncased',
        image_model='resnet50',
        fusion_type='early',
        feature_dim=512,
        freeze_encoders=True,
        dropout=0.3
    ).to(device)
    
    # åŠ è½½checkpoint
    if os.path.exists(checkpoint_path):
        print(f"åŠ è½½checkpoint: {checkpoint_path}")
        model, _, _, _ = load_checkpoint(model, None, checkpoint_path)
    else:
        print(f"âš ï¸ Checkpoint not found: {checkpoint_path}")
        print("ä½¿ç”¨æœªè®­ç»ƒçš„æ¨¡å‹è¿›è¡Œåˆ†æï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰")
    
    # åˆ†æbad cases
    print(f"\nåˆ†æ{args.split}é›†é”™è¯¯æ ·æœ¬...")
    bad_cases, stats = analyze_bad_cases(model, target_loader, device)
    
    # ä¿å­˜ç»“æœ
    output_path = f'analysis_results/bad_cases_{args.split}.csv'
    save_bad_cases(bad_cases, output_path)
    
    # æ‰“å°æŠ¥å‘Š
    print_analysis_report(stats, bad_cases)
    
    # ç”Ÿæˆä¼˜åŒ–å»ºè®®
    print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    
    if args.split == 'train':
        print("  âœ“ å¯ä»¥å®‰å…¨åœ°ä½¿ç”¨è¿™äº›bad casesè¿›è¡Œæ•°æ®å¢å¼º")
        print("  âœ“ è¿è¡Œ: python augment_bad_cases.py --input analysis_results/bad_cases_train.csv")
    else:
        print("  âš ï¸ è¿™äº›æ˜¯éªŒè¯é›†çš„bad casesï¼Œä»…ä¾›åˆ†æå‚è€ƒ")
        print("  âš ï¸ ä¸è¦ä½¿ç”¨è¿™äº›æ•°æ®è¿›è¡Œè®­ç»ƒæˆ–å¢å¼ºï¼")
    
    if stats['high_confidence_errors'] > stats['total_errors'] * 0.3:
        print("\n  âš ï¸ é«˜ç½®ä¿¡åº¦é”™è¯¯è¾ƒå¤š â†’ å»ºè®®:")
        print("     - æ£€æŸ¥æ•°æ®æ ‡æ³¨è´¨é‡")
        print("     - å¯¹é«˜ç½®ä¿¡åº¦é”™è¯¯æ ·æœ¬åšæ•°æ®å¢å¼º")
        print("     - è€ƒè™‘å¢åŠ æ¨¡å‹å®¹é‡")
    
    confusion = stats['confusion_matrix']
    if confusion.get('neutral â†’ positive', 0) + confusion.get('neutral â†’ negative', 0) > stats['total_errors'] * 0.3:
        print("\n  âš ï¸ neutralç±»åˆ«å®¹æ˜“è¯¯åˆ¤ â†’ å»ºè®®:")
        print("     - å¢åŠ neutralæ ·æœ¬çš„æ•°æ®å¢å¼º")
        print("     - è°ƒæ•´ç±»åˆ«æƒé‡")
        print("     - ä½¿ç”¨focal loss")


if __name__ == '__main__':
    os.makedirs('analysis_results', exist_ok=True)
    main()
